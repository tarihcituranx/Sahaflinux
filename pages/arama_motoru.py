import streamlit as st
import requests
from PIL import Image
from io import BytesIO
import re

# --- SAYFA AYARLARI ---
st.set_page_config(
    page_title="Brave Destekli Sahaf",
    page_icon="ğŸ¦",
    layout="wide"
)

# --- API ANAHTARI KONTROLÃœ ---
try:
    API_KEY = st.secrets["BRAVE_API_KEY"]
except:
    st.error("âš ï¸ API AnahtarÄ± bulunamadÄ±! LÃ¼tfen .streamlit/secrets.toml dosyasÄ±nÄ± oluÅŸturun.")
    st.stop()

# --- TEMÄ°ZLÄ°K FONKSÄ°YONU (YENÄ° EKLENDÄ°) ---
def clean_ocr_text(text):
    """
    OCR metinlerindeki bozukluklarÄ± Python ile temizler.
    Yapay zeka kullanmaz, kural tabanlÄ± Ã§alÄ±ÅŸÄ±r.
    """
    if not text:
        return ""

    # 1. Tire (-) ile bÃ¶lÃ¼nmÃ¼ÅŸ kelimeleri birleÅŸtir (Ã–rn: "da- vasÄ±" -> "davasÄ±")
    # Hem satÄ±r sonu (-) hem de kelime ortasÄ± yanlÄ±ÅŸ boÅŸluklu tireleri yakalar.
    text = re.sub(r'-\s+', '', text)

    # 2. Fazla boÅŸluklarÄ±, tab'larÄ± ve yeni satÄ±rlarÄ± tek bir boÅŸluÄŸa indir
    text = re.sub(r'\s+', ' ', text)

    # 3. YaygÄ±n OCR hatalarÄ±nÄ± dÃ¼zelt (Dictionary YÃ¶ntemi)
    # Eski taramalarda harfler bazen ayrÄ± ayrÄ± Ã§Ä±kar.
    replacements = {
        " v e ": " ve ",
        " b ir ": " bir ",
        " b u ": " bu ",
        " d e ": " de ",
        " d a ": " da ",
        " n e ": " ne ",
        " i Ã§ i n ": " iÃ§in ",
        " o l a n ": " olan ",
        " Ä± ": "Ä±", 
        " i ": "i",
    }
    
    for old, new in replacements.items():
        text = text.replace(old, new)

    # 4. Metnin baÅŸÄ±ndaki ve sonundaki boÅŸluklarÄ± temizle
    return text.strip()

# --- DÄ°ÄER FONKSÄ°YONLAR ---

def search_brave(keyword, count=20):
    """Brave Search API kullanarak arama yapar"""
    url = "https://api.search.brave.com/res/v1/web/search"
    query = f'site:gastearsivi.com {keyword}'
    
    headers = {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": API_KEY
    }
    
    params = {
        "q": query,
        "count": count,
        "country": "tr",
        "safesearch": "off"
    }
    
    found_items = []
    
    try:
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code == 200:
            data = response.json()
            if "web" in data and "results" in data["web"]:
                results = data["web"]["results"]
                
                for r in results:
                    page_url = r["url"]
                    # AÃ§Ä±klamayÄ± alÄ±yoruz (Bazen description boÅŸ olabilir)
                    raw_desc = r.get("description", "") or r.get("title", "")
                    
                    # --- OTOMATÄ°K TEMÄ°ZLÄ°K BURADA YAPILIYOR ---
                    final_desc = clean_ocr_text(raw_desc)
                    
                    match = re.search(r"gazete\/([^\/]+)\/(\d{4}-\d{2}-\d{2})\/(\d+)", page_url)
                    
                    if match:
                        gid = match.group(1)
                        date_str = match.group(2)
                        page_num = match.group(3)
                        g_name = gid.replace("_", " ").title()
                        
                        found_items.append({
                            "id": gid,
                            "name": g_name,
                            "date": date_str,
                            "page": page_num,
                            "desc": final_desc, # ArtÄ±k temizlenmiÅŸ metni kaydediyoruz
                            "url": page_url
                        })
        elif response.status_code == 429:
            st.warning("HÄ±z sÄ±nÄ±rÄ± aÅŸÄ±ldÄ±.")
        elif response.status_code == 401:
            st.error("API AnahtarÄ± geÃ§ersiz.")
            
    except Exception as e:
        st.error(f"Hata: {e}")
        
    return found_items

def get_cdn_image(gid, date_str, page_num):
    """Resmi CDN'den Ã§eker"""
    base_url = "https://dzp35pmd4yqn4.cloudfront.net"
    url = f"{base_url}/thumbnails/{gid}/{date_str}-{page_num}-thumbnail250.jpg"
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=2)
        if r.status_code == 200:
            return Image.open(BytesIO(r.content))
    except:
        pass
    return None

def download_pdf(gid, date_str, page_num):
    """PDF Ä°ndirir"""
    url = f"https://dzp35pmd4yqn4.cloudfront.net/sayfalar/{gid}/{date_str}-{page_num}.jpg"
    try:
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            img = Image.open(BytesIO(r.content)).convert("L")
            pdf_buffer = BytesIO()
            img.save(pdf_buffer, format="PDF", resolution=100.0, quality=85)
            pdf_buffer.seek(0)
            return pdf_buffer
    except:
        return None
    return None

# --- SESSION STATE ---
if 'search_results' not in st.session_state:
    st.session_state.search_results = []
if 'pdf_cache' not in st.session_state:
    st.session_state.pdf_cache = {}

# --- ARAYÃœZ ---

st.title("ğŸ¦ Brave Destekli Dijital Sahaf")
st.markdown("Google'Ä±n engellerine takÄ±lmadan, **Brave API** gÃ¼cÃ¼yle arÅŸivde arama yapÄ±n.")

with st.sidebar:
    st.header("Arama AyarlarÄ±")
    keyword = st.text_input("Anahtar Kelime", placeholder="Ã–rn: 10 KasÄ±m, Hatay...")
    count_slider = st.slider("SonuÃ§ SayÄ±sÄ±", 10, 50, 20)
    
    if st.button("ARA ğŸ”", type="primary"):
        if keyword:
            with st.spinner("Brave arÅŸivi tarÄ±yor ve metinleri temizliyor..."):
                st.session_state.search_results = search_brave(keyword, count_slider)
                st.session_state.pdf_cache = {} 
        else:
            st.warning("LÃ¼tfen bir kelime girin.")
    
    st.info("Brave API, ayda 2000 aramaya kadar Ã¼cretsizdir.")

# SONUÃ‡LARI GÃ–STER
results = st.session_state.search_results

if results:
    st.success(f"âœ… {len(results)} sonuÃ§ bulundu.")
    st.markdown("---")
    
    for item in results:
        with st.container():
            c1, c2 = st.columns([1, 4])
            
            with c1:
                img = get_cdn_image(item['id'], item['date'], item['page'])
                if img:
                    st.image(img, use_container_width=True)
                else:
                    st.image("https://placehold.co/200x300?text=Resim+Yok", use_container_width=True)
                    
            with c2:
                st.subheader(f"{item['name']} - {item['date']}")
                st.caption(f"Sayfa: {item['page']}")
                
                # TEMÄ°ZLENMÄ°Å METÄ°N BURADA GÃ–STERÄ°LÄ°YOR
                st.write(f"_{item['desc']}_")
                
                col_dl, col_go = st.columns([1, 3])
                
                unique_id = f"{item['id']}_{item['date']}_{item['page']}"
                
                with col_dl:
                    if unique_id not in st.session_state.pdf_cache:
                        if st.button(f"ğŸ“¥ PDF HazÄ±rla", key=f"btn_{unique_id}"):
                            with st.spinner("PDF DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor..."):
                                pdf_data = download_pdf(item['id'], item['date'], item['page'])
                                if pdf_data:
                                    st.session_state.pdf_cache[unique_id] = pdf_data
                                    st.rerun()
                                else:
                                    st.error("Dosya alÄ±namadÄ±.")
                    else:
                        fname = f"{item['name']}_{item['date']}_S{item['page']}.pdf"
                        st.download_button(
                            label="ğŸ’¾ PDF Ä°NDÄ°R",
                            data=st.session_state.pdf_cache[unique_id],
                            file_name=fname,
                            mime="application/pdf",
                            key=f"dl_{unique_id}",
                            type="primary"
                        )

                with col_go:
                    st.markdown(f"[Orjinal Sayfaya Git]({item['url']})")
                    
            st.divider()
