import time
import re
import io
from collections import defaultdict
from urllib.parse import urljoin, quote
from datetime import datetime, timezone, timedelta

import requests
import urllib3
import streamlit as st
from bs4 import BeautifulSoup
from groq import Groq

try:
    import PyPDF2
    PDF_DESTEKLI = True
except ImportError:
    PDF_DESTEKLI = False

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AYARLAR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANASAYFA_URL      = "https://kamuilan.sbb.gov.tr/"
GROQ_API_KEY      = st.secrets["GROQ_API_KEY"]
GROQ_MODEL        = "llama-3.3-70b-versatile"
RATE_LIMIT_SANIYE = 4
TZ_TURKIYE        = timezone(timedelta(hours=3))

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/145.0.0.0 Mobile Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,"
              "image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "DNT": "1",
    "Upgrade-Insecure-Requests": "1",
    "Connection": "keep-alive",
}

TURKCE_AYLAR = {
    "ocak":1,"ÅŸubat":2,"mart":3,"nisan":4,"mayÄ±s":5,
    "haziran":6,"temmuz":7,"aÄŸustos":8,"eylÃ¼l":9,
    "ekim":10,"kasÄ±m":11,"aralÄ±k":12,
    "subat":2,"mayis":5,"agustos":8,"eylul":9,"kasim":11,"aralik":12,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# YARDIMCI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize(m):
    return (m.lower()
            .replace("Ä±","i").replace("ÄŸ","g").replace("Ã¼","u")
            .replace("ÅŸ","s").replace("Ã¶","o").replace("Ã§","c")
            .replace("Ä°","i").replace("Ä","g").replace("Ãœ","u")
            .replace("Å","s").replace("Ã–","o").replace("Ã‡","c"))

def simdi_tr():
    return datetime.now(TZ_TURKIYE)

def session_olustur():
    """ASP.NET session cookie alÄ±r."""
    s = requests.Session()
    s.headers.update(HEADERS)
    try:
        s.get(ANASAYFA_URL, verify=False, timeout=15)
    except Exception:
        pass
    return s

def get_session():
    if "http_session" not in st.session_state:
        st.session_state["http_session"] = session_olustur()
    return st.session_state["http_session"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VERÄ° Ã‡EKME
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def tarih_parse(metin):
    """'21 Åubat' veya '21 Åubat 2026' â†’ (gun, ay, yil)"""
    metin = metin.strip()
    m = re.match(r"(\d{1,2})\s+(\w+)(?:\s+(\d{4}))?", metin)
    if not m:
        return None
    gun = int(m.group(1))
    ay_str = normalize(m.group(2))
    yil = int(m.group(3)) if m.group(3) else simdi_tr().year
    ay = TURKCE_AYLAR.get(ay_str)
    if not ay:
        return None
    return (yil, ay, gun)

def ilanlarÄ±_cek_raw():
    """
    Ana sayfadan tÃ¼m ilanlarÄ± Ã§eker.
    Her ilan: {kurum, baslik, basvuru_tarihi, link, tarih_tuple, logo_url}
    """
    session = get_session()
    try:
        resp = session.get(ANASAYFA_URL, verify=False, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        st.error(f"Sayfa Ã§ekilemedi: {e}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    ilanlar = []
    gorulen = set()
    son_tarih = None

    # Sayfa yapÄ±sÄ±: tarih baÅŸlÄ±klarÄ± + altÄ±nda ilan kartlarÄ±
    # Tarihler genellikle "21 Åubat" formatÄ±nda ayrÄ± bir element
    # Ä°lan kartlarÄ±: kurum adÄ± + ilan baÅŸlÄ±ÄŸÄ± + baÅŸvuru tarihi + link

    # TÃ¼m a taglerini tara â€” ilan detay linkleri ilanDetay.aspx iÃ§eriyor
    for eleman in soup.find_all(True):
        tag = eleman.name

        # Tarih baÅŸlÄ±klarÄ±nÄ± yakala (21 Åubat, 20 Åubat gibi)
        metin = eleman.get_text(strip=True)
        if tag in ("h3","h4","h5","span","div","p"):
            t = tarih_parse(metin)
            if t and len(metin) < 20:
                son_tarih = t
                continue

        # Ä°lan linklerini yakala
        if tag == "a":
            href = eleman.get("href", "")
            if "ilanDetay.aspx" not in href:
                continue
            tam_url = urljoin(ANASAYFA_URL, href)
            if tam_url in gorulen:
                continue
            gorulen.add(tam_url)

            # Kart iÃ§eriÄŸini al - en yakÄ±n anlamlÄ± kapsayÄ±cÄ±ya Ã§Ä±k
            kapsayici = eleman
            for _ in range(5):
                p = kapsayici.parent
                if not p:
                    break
                metin_uzunluk = len(p.get_text(strip=True))
                if metin_uzunluk > 20:
                    kapsayici = p
                    break
                kapsayici = p

            kart_metin = kapsayici.get_text(" ", strip=True)

            # Logo URL
            logo = None
            img = kapsayici.find("img")
            if img and img.get("src"):
                logo = urljoin(ANASAYFA_URL, img["src"])

            # BaÅŸvuru tarihi aralÄ±ÄŸÄ± (parantez iÃ§inde genellikle)
            basvuru = ""
            tarih_m = re.search(r"\(([^)]+)\)", kart_metin)
            if tarih_m:
                basvuru = tarih_m.group(1).strip()

            # Kurum adÄ± ve ilan baÅŸlÄ±ÄŸÄ±
            # Genellikle BÃœYÃœK HARF kurum adÄ± + ilan baÅŸlÄ±ÄŸÄ±
            satirlar = [s for s in kart_metin.splitlines() if s.strip()]
            kurum = ""
            baslik = eleman.get_text(strip=True)

            # Ä°lanÄ±n link metni boÅŸsa kart metninden Ã§Ä±kar
            if not baslik or len(baslik) < 5:
                baslik = kart_metin[:120]

            ilanlar.append({
                "kurum"         : kurum,
                "baslik"        : baslik,
                "basvuru_tarihi": basvuru,
                "link"          : tam_url,
                "tarih"         : son_tarih,
                "logo_url"      : logo,
            })

    # EÄŸer yukarÄ±daki yaklaÅŸÄ±m az ilan dÃ¶ndÃ¼rÃ¼yorsa alternatif yÃ¶ntem
    if len(ilanlar) < 3:
        ilanlar = ilanlarÄ±_cek_alternatif(soup)

    return ilanlar


def ilanlarÄ±_cek_alternatif(soup):
    """
    Alternatif: sayfadaki tÃ¼m ilanDetay linklerini bul,
    her birinin kart yapÄ±sÄ±nÄ± farklÄ± ÅŸekilde oku.
    """
    ilanlar = []
    gorulen = set()
    son_tarih = None

    # Ã–nce tÃ¼m metni tara, tarih kalÄ±plarÄ±nÄ± bul
    for el in soup.find_all(string=re.compile(r"^\d{1,2}\s+\w+$")):
        t = tarih_parse(el.strip())
        if t:
            son_tarih = t

    for a in soup.find_all("a", href=re.compile(r"ilanDetay\.aspx")):
        href = a.get("href","")
        tam_url = urljoin(ANASAYFA_URL, href)
        if tam_url in gorulen:
            continue
        gorulen.add(tam_url)

        # Kart: linkin en yakÄ±n bÃ¼yÃ¼k kapsayÄ±cÄ±sÄ±
        kart = a
        for _ in range(8):
            parent = kart.parent
            if not parent:
                break
            if parent.name in ("li","article","div","td"):
                txt = parent.get_text(" ", strip=True)
                if len(txt) > 15:
                    kart = parent
                    break
            kart = parent

        kart_metin = kart.get_text(" ", strip=True)

        # Logo
        logo = None
        img = kart.find("img")
        if img:
            logo = urljoin(ANASAYFA_URL, img.get("src",""))

        # BaÅŸvuru tarihi
        basvuru = ""
        tarih_m = re.search(r"\(\s*(\d+\s+\w+\s*[-â€“]\s*\d+\s+\w+)\s*\)", kart_metin)
        if tarih_m:
            basvuru = tarih_m.group(1).strip()

        baslik = a.get_text(strip=True) or kart_metin[:100]

        ilanlar.append({
            "kurum"         : "",
            "baslik"        : baslik,
            "basvuru_tarihi": basvuru,
            "link"          : tam_url,
            "tarih"         : son_tarih,
            "logo_url"      : logo,
        })

    return ilanlar


def veri_guncelle():
    with st.spinner("ğŸ”„ Ä°lanlar gÃ¼ncelleniyor..."):
        # Session yenile
        st.session_state["http_session"] = session_olustur()
        ilanlar = ilanlarÄ±_cek_raw()
    st.session_state["ilanlar"]        = ilanlar
    st.session_state["son_guncelleme"] = simdi_tr()
    return ilanlar


def veri_yukle():
    simdi = simdi_tr()
    if "ilanlar" not in st.session_state:
        return veri_guncelle()
    son = st.session_state.get("son_guncelleme")
    if son and son.date() < simdi.date():
        return veri_guncelle()
    return st.session_state["ilanlar"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ä°LAN DETAY + PDF
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JENERIK_METINLER = {
    "dosyayi gorun", "goruntule", "goster", "indir", "tikla",
    "tiklayin", "download", "open", "view", "dosyayi indir", "ekler"
}

SAYFA_LINKLERI = re.compile(r"ilanDetay|arsiv|Default|javascript:|#|mailto:|tel:", re.I)

def dosya_linki_mi(href, base_url):
    hl = href.lower()
    for ext in (".pdf",".doc",".docx",".xls",".xlsx",".zip"):
        if hl.endswith(ext):
            return True
    kaliplar = ["getfile","dosyagetir","filedownload","download","/dosya/","/files/","/upload"]
    for k in kaliplar:
        if k in hl:
            return True
    if re.search(r"[?&](id|dosyaid|file|f)=\d+", hl):
        return True
    return False

def pdf_icerigi_cek(pdf_url, session):
    if not PDF_DESTEKLI:
        return ""
    try:
        resp = session.get(pdf_url, verify=False, timeout=15)
        resp.raise_for_status()
        reader = PyPDF2.PdfReader(io.BytesIO(resp.content))
        metin = ""
        for sayfa in reader.pages[:5]:
            metin += sayfa.extract_text() or ""
        return metin[:3000].strip()
    except Exception:
        return ""

def pdf_ad_bul(a_tag, href):
    from urllib.parse import unquote
    link_metni = a_tag.get_text(strip=True)
    norm_link  = normalize(link_metni)
    if link_metni and len(link_metni) > 4 and not any(j in norm_link for j in JENERIK_METINLER):
        return link_metni
    try:
        kapsayici = a_tag.parent
        for _ in range(5):
            if kapsayici is None: break
            metin = kapsayici.get_text(" ", strip=True)
            eslesme = re.search(r"([^\n]{3,80}[.]pdf)", metin, re.I)
            if eslesme:
                ad = eslesme.group(1).strip()
                if len(ad) < 100:
                    return ad
            kapsayici = kapsayici.parent
    except Exception:
        pass
    dosya = href.split("/")[-1].split("?")[0]
    try:
        dosya = unquote(dosya)
    except Exception:
        pass
    return dosya or "Belge"

def ilan_icerik_cek(url):
    session = get_session()
    try:
        resp = session.get(url, verify=False, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # TÃ¼m linkler (debug)
        tum_linkler = []
        for a in soup.find_all("a", href=True):
            metin = a.get_text(strip=True)
            href  = a["href"]
            if href and href not in ("#","javascript:void(0)"):
                tum_linkler.append({
                    "metin": metin[:60],
                    "href" : href[:120],
                    "tam"  : urljoin(url, href)[:150],
                })

        # Nav/footer temizle
        for tag in soup(["nav","header","footer","script","style"]):
            tag.decompose()

        # PDF / dosya linkleri
        pdf_listesi = []
        gorulen = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not href or href.startswith(("javascript:","mailto:","tel:")):
                continue
            if not dosya_linki_mi(href, url):
                continue
            tam_url = urljoin(url, href)
            if tam_url in gorulen:
                continue
            gorulen.add(tam_url)
            ad     = pdf_ad_bul(a, href)
            icerik = pdf_icerigi_cek(tam_url, session)
            pdf_listesi.append({"ad": ad, "url": tam_url, "icerik": icerik})

        satirlar = [s for s in soup.get_text("\n", strip=True).splitlines() if s.strip()]
        metin = "\n".join(satirlar[:400])

        return metin, pdf_listesi, tum_linkler

    except Exception as e:
        return f"Ä°Ã§erik alÄ±namadÄ±: {e}", [], []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GROQ AI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def groq_ozet(baslik, icerik, pdf_listesi=None):
    su_an   = time.time()
    bekleme = RATE_LIMIT_SANIYE - (su_an - st.session_state.get("son_groq_istegi", 0))
    if bekleme > 0:
        time.sleep(bekleme)
    try:
        client = Groq(api_key=GROQ_API_KEY)
        sistem = """Sen TÃ¼rkiye kamu kurumlarÄ±ndaki personel alÄ±m ilanlarÄ±nÄ± analiz eden bir uzmansÄ±n.

GÃ¶revin: Verilen ilan metnini AYNEN ve EKSÄ°KSÄ°Z analiz etmek.

KRÄ°TÄ°K KURALLAR:
1. Metinde geÃ§en TÃœM sayÄ±sal ve spesifik bilgileri yaz: Ã¶lÃ§Ã¼ler, adetler, saatler, tarihler, adresler, banka adlarÄ±, form adlarÄ± vb.
2. Listeler varsa (Ã¶rn: istenen belgeler, aranan nitelikler) HER maddeyi ayrÄ± satÄ±rda numaralÄ± yaz.
3. Metinde OLMAYAN hiÃ§bir bilgiyi uydurma.
4. YanÄ±tÄ±nÄ± TÃ¼rkÃ§e ver.

Åu baÅŸlÄ±klarÄ± kullan (bilgi yoksa o baÅŸlÄ±ÄŸÄ± atla):

ğŸ“‹ **Ä°lan Ã–zeti**
ğŸ›ï¸ **Kurum**
ğŸ¯ **Aranan Pozisyon(lar)**
ğŸ”¢ **AlÄ±nacak KiÅŸi SayÄ±sÄ±**
ğŸ“š **Aranan Åartlar / Nitelikler**
ğŸ“‹ **Ä°stenen Belgeler**
ğŸ“… **Ã–nemli Tarihler**
ğŸ“ **BaÅŸvuru Bilgileri**
âš ï¸ **Dikkat Edilmesi Gerekenler**
â¬‡ï¸ **Ekli Dosyalar** (varsa)"""

        ek_metin = ""
        if pdf_listesi:
            ek_metin = "\n\n--- EKLÄ° DOSYALAR ---\n"
            for p in pdf_listesi:
                ek_metin += f"\nDosya: {p['ad']}\nURL: {p['url']}\n"
                if p["icerik"]:
                    ek_metin += f"Ä°Ã§erik:\n{p['icerik'][:1500]}\n"

        yanit = client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role":"system","content":sistem},
                {"role":"user","content":f"Ä°lan BaÅŸlÄ±ÄŸÄ±: {baslik}\n\nÄ°lan Ä°Ã§eriÄŸi:\n{icerik}{ek_metin}"},
            ],
            temperature=0.1,
            max_tokens=2500,
        )
        st.session_state["son_groq_istegi"] = time.time()
        return yanit.choices[0].message.content
    except Exception as e:
        hata = str(e)
        if "rate_limit" in hata.lower():
            return "â³ Groq rate limit aÅŸÄ±ldÄ±. LÃ¼tfen birkaÃ§ saniye bekleyip tekrar deneyin."
        return f"âŒ Groq hatasÄ±: {hata}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FAVORÄ°LER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def favori_toggle(link):
    favs = st.session_state.setdefault("kamu_favoriler", set())
    if link in favs:
        favs.discard(link)
    else:
        favs.add(link)

def favori_mi(link):
    return link in st.session_state.get("kamu_favoriler", set())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AKILLI ARAMA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def arama_eslesiyor(arama_metni, ilan):
    if not arama_metni:
        return True
    hedef = normalize(ilan["baslik"] + " " + ilan.get("kurum",""))
    kelimeler = normalize(arama_metni).split()
    return all(k in hedef for k in kelimeler)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CSS = """
<style>
.kamu-kart {
    background: #1a1d2e;
    border-left: 4px solid #7c3aed;
    border-radius: 8px;
    padding: 14px 18px;
    margin-bottom: 4px;
}
.kamu-baslik {
    font-size: 15px;
    font-weight: 600;
    color: #e8eaf0;
    margin-bottom: 4px;
    line-height: 1.4;
}
.kamu-meta {
    font-size: 12px;
    color: #9aa0b4;
    margin-top: 3px;
}
.kamu-tarih {
    font-size: 12px;
    color: #f59e0b;
    font-weight: 500;
}
.arama-ipucu {
    font-size: 11px;
    color: #6b7280;
    margin-top: 2px;
}
</style>
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KART
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ilan_karti_goster(d, idx):
    fav      = favori_mi(d["link"])
    fav_ikon = "â­" if fav else "â˜†"

    tarih_str = ""
    if d.get("basvuru_tarihi"):
        tarih_str = f"ğŸ“… {d['basvuru_tarihi']}"

    kurum_str = f"ğŸ›ï¸ {d['kurum']}  " if d.get("kurum") else ""

    st.markdown(f"""
    <div class="kamu-kart">
        <div class="kamu-baslik">{d['baslik']}</div>
        <div class="kamu-meta">{kurum_str}<span class="kamu-tarih">{tarih_str}</span></div>
    </div>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([2, 2, 1])

    with col1:
        st.link_button("ğŸ”— Ä°lana Git", url=d["link"], use_container_width=True)

    anahtar   = f"kamu_ozet_{idx}"
    pdf_key   = f"kamu_pdf_{idx}"
    link_key  = f"kamu_lnk_{idx}"

    with col2:
        if st.button("ğŸ¤– AI Ã–zet", key=f"kamu_btn_{idx}", use_container_width=True):
            with st.spinner("ğŸ“– Ä°lan okunuyor..."):
                icerik, pdf_listesi, tum_linkler = ilan_icerik_cek(d["link"])
                st.session_state[pdf_key]  = pdf_listesi
                st.session_state[link_key] = tum_linkler
            with st.spinner("ğŸ¤– AI Ã¶zetleniyor..."):
                ozet = groq_ozet(d["baslik"], icerik, pdf_listesi)
                st.session_state[anahtar] = ozet

    with col3:
        if st.button(f"{fav_ikon} Favori", key=f"kamu_fav_{idx}", use_container_width=True):
            favori_toggle(d["link"])
            st.rerun()

    if anahtar in st.session_state:
        with st.expander("ğŸ“„ AI Ã–zeti", expanded=True):
            st.markdown(st.session_state[anahtar])

        # PDF butonlarÄ±
        pdf_listesi = st.session_state.get(pdf_key, [])
        if pdf_listesi:
            st.markdown("**ğŸ“ Ekli Belgeler â€” Ä°ndir:**")
            for pdf in pdf_listesi:
                col_ad, col_btn = st.columns([4, 1])
                with col_ad:
                    st.markdown(
                        f"<div style='padding:5px 0;font-size:14px;'>ğŸ“„ {pdf['ad']}</div>",
                        unsafe_allow_html=True,
                    )
                with col_btn:
                    st.link_button("â¬‡ï¸ Ä°ndir", url=pdf["url"], use_container_width=True)
            st.markdown("")
        else:
            tum_linkler = st.session_state.get(link_key, [])
            if tum_linkler:
                with st.expander("ğŸ” Sayfadaki TÃ¼m Linkler (debug â€” PDF bulunamadÄ±)", expanded=False):
                    for lnk in tum_linkler:
                        st.markdown(f"**Metin:** `{lnk['metin']}` | **href:** `{lnk['href']}`")

        if st.button("âœ–ï¸ Kapat", key=f"kamu_kapat_{idx}"):
            for k in [anahtar, pdf_key, link_key]:
                if k in st.session_state:
                    del st.session_state[k]
            st.rerun()

    st.divider()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SIDEBAR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sidebar_filtre():
    st.sidebar.title("ğŸ“‹ Kamu Personel\nAlÄ±m Ä°lanlarÄ±")
    st.sidebar.markdown("---")

    st.sidebar.subheader("ğŸ”¤ AkÄ±llÄ± Arama")
    arama = st.sidebar.text_input(
        "Kurum veya ilan baÅŸlÄ±ÄŸÄ±nda ara:",
        placeholder="Ã¶rn: sahil gÃ¼venlik, mÃ¼hendis",
        key="kamu_arama",
    )
    st.sidebar.markdown(
        "<div class='arama-ipucu'>ğŸ’¡ Birden fazla kelime yazabilirsin. "
        "TÃ¼rkÃ§e karakter fark etmez.</div>",
        unsafe_allow_html=True,
    )

    st.sidebar.markdown("---")
    sadece_favori = st.sidebar.checkbox(
        f"â­ Sadece Favoriler ({len(st.session_state.get('kamu_favoriler', set()))})",
        key="kamu_sadece_favori",
    )

    # Kullanma KÄ±lavuzu
    st.sidebar.markdown("---")
    with st.sidebar.expander("ğŸ“– NasÄ±l KullanÄ±lÄ±r?", expanded=False):
        st.sidebar.markdown("""
**ğŸ”„ Verileri GÃ¼ncelle**
Siteyi anlÄ±k tarar, yeni ilanlarÄ± Ã§eker.
Her gÃ¼n otomatik gÃ¼ncellenir.

---

**ğŸ”¤ AkÄ±llÄ± Arama**
Kurum adÄ± veya ilan baÅŸlÄ±ÄŸÄ±nda arama yapar.
Birden fazla kelime yazabilirsin â€” hepsi
eÅŸleÅŸmeli. TÃ¼rkÃ§e karakter takÄ±lmaz.

---

**â­ Favoriler**
â˜† Favori butonuna basÄ±nca ilan yÄ±ldÄ±zlanÄ±r.
"Sadece Favoriler" ile sadece onlarÄ± gÃ¶rÃ¼rsÃ¼n.
Favoriler sayfanÄ±n Ã¼stÃ¼nde her zaman gÃ¶rÃ¼nÃ¼r.

---

**ğŸ¤– AI Ã–zet**
Ä°lanÄ± otomatik okuyup Ã¶zetler:
pozisyon, ÅŸartlar, tarihler, belgeler.
Varsa ekli PDF'leri de indirme butonu ile gÃ¶sterir.
        """)

    return {
        "arama"        : arama.strip(),
        "sadece_favori": sadece_favori,
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FÄ°LTRELE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ilan_filtrele(ilanlar, filtre):
    sonuc = ilanlar
    if filtre["sadece_favori"]:
        favs  = st.session_state.get("kamu_favoriler", set())
        sonuc = [d for d in sonuc if d["link"] in favs]
    if filtre["arama"]:
        sonuc = [d for d in sonuc if arama_eslesiyor(filtre["arama"], d)]
    return sonuc

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.set_page_config(
        page_title="Kamu Personel AlÄ±m Ä°lanlarÄ±",
        page_icon="ğŸ“‹",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.markdown(CSS, unsafe_allow_html=True)

    for k, v in [("son_groq_istegi", 0), ("kamu_favoriler", set())]:
        if k not in st.session_state:
            st.session_state[k] = v

    if not PDF_DESTEKLI:
        st.info("â„¹ï¸ PDF iÃ§erikleri iÃ§in `pip install PyPDF2` kurabilirsin.")

    # BaÅŸlÄ±k + GÃ¼ncelle
    col_b, col_btn = st.columns([5, 1])
    with col_b:
        st.title("ğŸ“‹ Kamu Personeli AlÄ±m Ä°lanlarÄ±")
    with col_btn:
        st.write("")
        if st.button("ğŸ”„ Verileri GÃ¼ncelle", use_container_width=True):
            st.cache_data.clear()
            veri_guncelle()
            st.success("GÃ¼ncellendi!")

    ilanlar = veri_yukle()

    son = st.session_state.get("son_guncelleme")
    if son:
        st.caption(f"ğŸ“¡ Kaynak: kamuilan.sbb.gov.tr Â· Son gÃ¼ncelleme: {son.strftime('%d.%m.%Y %H:%M')}")

    # VarsayÄ±lan gÃ¶rÃ¼nÃ¼m butonu
    if st.button("ğŸ  VarsayÄ±lan GÃ¶rÃ¼nÃ¼me DÃ¶n", use_container_width=True):
        for k in ["kamu_arama", "kamu_sadece_favori"]:
            if k in st.session_state:
                del st.session_state[k]
        st.rerun()

    if not ilanlar:
        st.error("Ä°lanlar yÃ¼klenemedi. LÃ¼tfen 'Verileri GÃ¼ncelle' butonuna tÄ±klayÄ±n.")
        return

    filtre   = sidebar_filtre()
    filtreli = ilan_filtrele(ilanlar, filtre)

    # Ä°statistik
    c1, c2, c3 = st.columns(3)
    c1.metric("ğŸ“‹ Toplam Ä°lan", len(ilanlar))
    c2.metric("ğŸ” GÃ¶sterilen",  len(filtreli))
    c3.metric("â­ Favoriler",   len(st.session_state.get("kamu_favoriler", set())))

    st.markdown("---")

    if not filtreli:
        st.info("SeÃ§ilen filtrelere uygun ilan bulunamadÄ±.")
        return

    # â”€â”€ Favoriler â€” her zaman Ã¼stte â”€â”€
    favs = st.session_state.get("kamu_favoriler", set())
    favori_ilanlar = [d for d in ilanlar if d["link"] in favs]
    if favori_ilanlar:
        with st.expander(f"â­ Favorilerim â€” {len(favori_ilanlar)} ilan", expanded=True):
            for d in favori_ilanlar:
                ilan_karti_goster(d, hash(d["link"] + "_fav"))
        st.markdown("---")

    # â”€â”€ TÃ¼m ilanlar (tarih gruplama YOK â€” site zaten gÃ¼ncel ilanlar) â”€â”€
    with st.expander(f"ğŸ“‹ GÃ¼ncel Ä°lanlar â€” {len(filtreli)} ilan", expanded=True):
        for d in filtreli:
            ilan_karti_goster(d, hash(d["link"]))

if __name__ == "__main__":
    main()
