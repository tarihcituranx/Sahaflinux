import streamlit as st
import time
import requests
import re
import os
import pandas as pd
from io import BytesIO
import zipfile
from bs4 import BeautifulSoup

# --- SELENIUM AYARLARI ---
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    from webdriver_manager.core.os_manager import ChromeType
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

st.set_page_config(page_title="Harici Kaynaklar", page_icon="ðŸŒ", layout="wide")

# GERÄ° DÃ–N BUTONU
with st.sidebar:
    st.title("âš™ï¸ Kontrol Paneli")
    # Dosya adÄ±n 'Ana_Sayfa.py' ise burasÄ± doÄŸru
    st.page_link("app.py", label="â¬…ï¸ Gazete ArÅŸivine DÃ¶n", icon="â†©ï¸")
    st.markdown("---")

st.title("ðŸŒ Harici Kaynaklar & CanlÄ± Arama")

# --- TARAYICI BAÅžLATMA FONKSÄ°YONU ---
def baslat_driver():
    options = Options()
    options.add_argument("--headless") # Cloud iÃ§in ÅŸart
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    
    try:
        service = Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install())
        return webdriver.Chrome(service=service, options=options)
    except:
        service = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=service, options=options)

# --- HTU VERÄ° Ã‡EKME FONKSÄ°YONU (REQUESTS Ä°LE - DAHA HIZLI) ---
@st.cache_data(ttl=3600) # 1 saat Ã¶nbellekte tutar, sÃ¼rekli siteye gitmez
def htu_verilerini_getir():
    base_url = "https://www.tufs.ac.jp/common/fs/asw/tur/htu/"
    pages = ["list1.html", "list2.html"] # A-L ve M-Z listeleri
    
    all_data = []
    
    for page in pages:
        try:
            r = requests.get(base_url + page)
            r.encoding = 'utf-8'
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'html.parser')
                # Tabloyu bul (id='tblist')
                table = soup.find('table', id='tblist')
                if table:
                    rows = table.find_all('tr')
                    for row in rows:
                        cols = row.find_all('td')
                        # YapÄ±: [No] [HTU No] [Title (Link)] [Desc]
                        if len(cols) >= 4:
                            htu_no = cols[1].get_text(strip=True)
                            title_col = cols[2]
                            desc_col = cols[3]
                            
                            title_text = title_col.get_text(strip=True)
                            desc_text = desc_col.get_text(strip=True)
                            
                            # Linki al ve tam adrese Ã§evir
                            link_tag = title_col.find('a')
                            if link_tag and link_tag.has_attr('href'):
                                full_link = base_url + link_tag['href']
                            else:
                                full_link = ""
                                
                            # BaÅŸlÄ±k satÄ±rlarÄ±nÄ± (A, B, C...) atla
                            if "HTU no." in htu_no or not htu_no:
                                continue
                                
                            all_data.append({
                                "HTU NO.": htu_no,
                                "BAÅžLIK": title_text,
                                "AÃ‡IKLAMA": desc_text,
                                "LINK": full_link
                            })
        except Exception as e:
            st.error(f"Veri Ã§ekme hatasÄ± ({page}): {e}")
            
    return pd.DataFrame(all_data)

# --- DJVU Ä°NDÄ°RME VE DÃ–NÃœÅžTÃœRME ---
def download_and_process_djvu(url, filename):
    try:
        # 1. DosyayÄ± Ä°ndir
        r = requests.get(url, stream=True)
        if r.status_code != 200:
            return None, "Dosya indirilemedi."
        
        djvu_content = r.content
        
        # 2. PDF'e Ã§evirmeyi dene (Sunucuda araÃ§ varsa)
        # Not: Streamlit Cloud'da 'ddjvu' komutu packages.txt ile yÃ¼klenir.
        # Ancak iÅŸlemci gÃ¼cÃ¼ yetmeyebilir veya dosya Ã§ok bÃ¼yÃ¼k olabilir.
        # Bu yÃ¼zden Ã¶nce basitÃ§e orijinali verelim, opsiyonel Ã§eviri yapalÄ±m.
        
        return djvu_content, "OK"
        
    except Exception as e:
        return None, str(e)

# --- SEKMELER ---
tab1, tab2 = st.tabs(["ðŸ“œ HTU ArÅŸivi (CanlÄ± Tarama)", "ðŸ¤– DergiPark Botu"])

# --------------------------------------------------------
# SEKME 1: HTU ARÅžÄ°VÄ° (CANLI ARAMA VE Ä°NDÄ°RME)
# --------------------------------------------------------
with tab1:
    st.header("ðŸ“œ HTU Dijital SÃ¼reli YayÄ±nlar")
    st.info("Bu modÃ¼l, Tokyo Ãœniversitesi'nin (HTU) A-Z listelerini canlÄ± tarar.")

    # Arama Kutusu
    col1, col2 = st.columns([4,1])
    search_term = col1.text_input("YayÄ±n AdÄ± veya HTU No Ara:", placeholder="Ã–rn: 11 Temmuz...")
    
    # Verileri Ã‡ek
    with st.spinner("VeritabanÄ± gÃ¼ncelleniyor..."):
        df = htu_verilerini_getir()
    
    if not df.empty:
        # Filtreleme
        if search_term:
            filtered_df = df[
                df['BAÅžLIK'].str.contains(search_term, case=False) | 
                df['HTU NO.'].str.contains(search_term, case=False) |
                df['AÃ‡IKLAMA'].str.contains(search_term, case=False)
            ]
        else:
            filtered_df = df

        st.write(f"Toplam {len(filtered_df)} sonuÃ§ bulundu.")
        
        # Tabloyu GÃ¶ster (SeÃ§im Kutulu)
        # Ã–nce 'SeÃ§' sÃ¼tunu ekle
        filtered_df.insert(0, "SeÃ§", False)
        
        edited_df = st.data_editor(
            filtered_df,
            column_config={
                "SeÃ§": st.column_config.CheckboxColumn("Ä°ndir", default=False),
                "LINK": st.column_config.LinkColumn("DoÄŸrudan Link"),
            },
            hide_index=True,
            use_container_width=True,
            key="htu_editor"
        )
        
        # Ä°ndirme Butonu
        selected_rows = edited_df[edited_df["SeÃ§"] == True]
        
        if not selected_rows.empty:
            st.divider()
            st.success(f"âœ… {len(selected_rows)} yayÄ±n seÃ§ildi.")
            
            if st.button("ðŸ“¦ SeÃ§ilenleri Ä°ndir (ZIP)", type="primary"):
                progress_bar = st.progress(0)
                zip_buffer = BytesIO()
                
                with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zf:
                    for idx, row in enumerate(selected_rows.itertuples()):
                        link = row.LINK
                        title = row.BAÅžLIK
                        safe_title = re.sub(r'[\\/*?:"<>|]', "", title)[:40] # Dosya adÄ± temizliÄŸi
                        
                        if link.endswith(".djvu"):
                            st.toast(f"Ä°ndiriliyor: {safe_title}...")
                            content, msg = download_and_process_djvu(link, safe_title)
                            
                            if content:
                                # Orijinal DjVu dosyasÄ±nÄ± ekle
                                zf.writestr(f"{safe_title}.djvu", content)
                            else:
                                zf.writestr(f"{safe_title}_HATA.txt", f"Ä°ndirme hatasÄ±: {msg}")
                        else:
                            zf.writestr(f"{safe_title}_LINK.txt", f"Bu yayÄ±n bir klasÃ¶r veya sayfadÄ±r. Link: {link}")
                        
                        progress_bar.progress((idx + 1) / len(selected_rows))
                
                zip_buffer.seek(0)
                st.download_button(
                    label="ðŸ’¾ ZIP DosyasÄ±nÄ± Kaydet",
                    data=zip_buffer,
                    file_name="HTU_Secilenler.zip",
                    mime="application/zip"
                )
    else:
        st.warning("Veri Ã§ekilemedi. LÃ¼tfen baÄŸlantÄ±nÄ±zÄ± kontrol edin.")

# --------------------------------------------------------
# SEKME 2: DERGÄ°PARK BOTU (Mevcut Bot Kodu)
# --------------------------------------------------------
with tab2:
    st.header("ðŸ¤– DergiPark Makale AvcÄ±sÄ±")
    
    if not SELENIUM_AVAILABLE:
        st.error("Selenium kÃ¼tÃ¼phanesi eksik!")
    else:
        with st.form("dp_form"):
            col1, col2 = st.columns([4,1])
            dp_kelime = col1.text_input("Makale Ara:", placeholder="Ã–rn: Ä°ttihat ve Terakki")
            dp_btn = col2.form_submit_button("ðŸš€ Botu BaÅŸlat")

        if dp_btn and dp_kelime:
            with st.status("ðŸ“¡ DergiPark taranÄ±yor...", expanded=True) as status:
                try:
                    driver = baslat_driver()
                    driver.get(f"https://dergipark.org.tr/tr/search?q={dp_kelime}&section=article")
                    
                    # Bekleme sÃ¼resi arttÄ±rÄ±ldÄ±
                    time.sleep(5)
                    
                    results = []
                    # CSS selector gÃ¼ncellendi
                    items = driver.find_elements("css selector", "h5.card-title a")
                    for item in items[:15]:
                        results.append({"title": item.text, "link": item.get_attribute("href")})
                    
                    driver.quit()
                    status.update(label="Bitti!", state="complete", expanded=False)
                    
                    if results:
                        st.success(f"{len(results)} makale bulundu.")
                        for r in results:
                            with st.expander(r['title']):
                                st.write(f"Link: {r['link']}")
                                if st.button("ðŸ“¥ PDF Ä°ndir", key=r['link']):
                                    try:
                                        headers = {'User-Agent': 'Mozilla/5.0'}
                                        req = requests.get(r['link'], headers=headers)
                                        # Basit PDF bulucu
                                        match = re.search(r'/tr/download/article-file/\d+', req.text)
                                        if match:
                                            pdf_url = "https://dergipark.org.tr" + match.group(0)
                                            pdf_data = requests.get(pdf_url, headers=headers).content
                                            clean_name = re.sub(r'[\\/*?:"<>|]', "", r['title'])[:30] + ".pdf"
                                            st.download_button("ðŸ’¾ Kaydet", pdf_data, clean_name, "application/pdf")
                                        else:
                                            st.error("PDF bulunamadÄ±.")
                                    except:
                                        st.error("Hata.")
                    else:
                        st.warning("SonuÃ§ yok.")
                except Exception as e:
                    st.error(f"Hata: {str(e)}")
