import streamlit as st
import requests
import pandas as pd
from io import BytesIO
import zipfile
from bs4 import BeautifulSoup
import urllib3
import cloudscraper
import re

# SSL UyarÄ±larÄ±nÄ± Sustur
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(page_title="Harici Kaynaklar", page_icon="ğŸŒ", layout="wide")

# --- YAN MENÃœ ---
with st.sidebar:
    st.title("âš™ï¸ Kontrol Paneli")
    st.success("âœ… HTU: Aktif")
    st.info("âœ… DergiPark: TarayÄ±cÄ± TabanlÄ± Ä°ndirme")
    st.markdown("---")
    st.caption("Bu modda doÄŸrulama ekranÄ± Ã§Ä±karsa, yeni sekmede kendiniz doÄŸrulayÄ±p indirebilirsiniz.")

# --- URL DÃœZELTÄ°CÄ° ---
def fix_url(link):
    if not link: return ""
    if not link.startswith("http"):
        if link.startswith("dergipark") or link.startswith("www"):
            link = "https://" + link
        elif link.startswith("/"):
            link = "https://dergipark.org.tr" + link
    return link

# ========================================================
# 1. HTU ARÅÄ°VÄ° (ZATEN Ã‡ALIÅAN KISIM)
# ========================================================
@st.cache_data(ttl=3600)
def htu_verilerini_getir():
    base_url = "https://www.tufs.ac.jp/common/fs/asw/tur/htu/"
    pages = ["list1.html", "list2.html"] 
    headers = {'User-Agent': 'Mozilla/5.0'}
    all_data = []
    
    for page in pages:
        full_url = base_url + page
        try:
            r = requests.get(full_url, headers=headers, timeout=30, verify=False)
            r.encoding = 'utf-8' 
            
            if r.status_code == 200:
                try: soup = BeautifulSoup(r.content, 'lxml')
                except: soup = BeautifulSoup(r.content, 'html.parser')

                all_rows = soup.find_all('tr')
                for row in all_rows:
                    try:
                        cols = row.find_all(['td', 'th'])
                        if len(cols) >= 3:
                            col_texts = [c.get_text(strip=True) for c in cols]
                            htu_no = col_texts[1]
                            if "HTU NO" in htu_no or not htu_no: continue
                            if len(htu_no) > 20: continue

                            baslik = col_texts[2] if len(cols) > 2 else ""
                            aciklama = col_texts[3] if len(cols) > 3 else ""

                            link_tag = cols[2].find('a')
                            raw_link = link_tag['href'] if link_tag and link_tag.has_attr('href') else ""
                            full_link = base_url + raw_link if raw_link and not raw_link.startswith("http") else raw_link
                            
                            all_data.append({
                                "HTU NO.": htu_no, "BAÅLIK": baslik,
                                "AÃ‡IKLAMA": aciklama, "LINK": full_link
                            })
                    except: continue
        except Exception as e: st.error(f"HTU HatasÄ±: {e}")
    
    df = pd.DataFrame(all_data)
    if not df.empty: df = df.drop_duplicates(subset=['HTU NO.'])
    return df

def download_and_process_djvu(url, filename):
    try:
        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, stream=True, verify=False)
        return (r.content, "OK") if r.status_code == 200 else (None, "BulunamadÄ±")
    except Exception as e: return None, str(e)


# ========================================================
# 2. DERGÄ°PARK (LÄ°NK BULUCU MOD)
# ========================================================

def search_dergipark_brave(keyword, count=15):
    try: api_key = st.secrets["BRAVE_API_KEY"]
    except: st.error("âš ï¸ Brave API AnahtarÄ± eksik!"); return []

    url = "https://api.search.brave.com/res/v1/web/search"
    query = f'site:dergipark.org.tr/tr/pub "{keyword}"'
    headers = {"Accept": "application/json", "Accept-Encoding": "gzip", "X-Subscription-Token": api_key}
    params = {"q": query, "count": count, "country": "tr"}
    
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            data = response.json()
            results = []
            if "web" in data and "results" in data["web"]:
                for item in data["web"]["results"]:
                    raw_link = item["url"]
                    clean_link = fix_url(raw_link)
                    if "dergipark.org.tr" in clean_link and "/pub/article/" not in clean_link:
                        results.append({
                            "title": item["title"],
                            "link": clean_link,
                            "desc": item.get("description", "")
                        })
            return results
    except Exception as e: st.error(f"Arama HatasÄ±: {e}")
    return []

def get_real_pdf_link(article_url):
    """
    Sadece linki bulur. Ä°ndirmeyi kullanÄ±cÄ±ya bÄ±rakÄ±r.
    """
    scraper = cloudscraper.create_scraper()
    try:
        # Sadece HTML'i Ã§ekip linki ayÄ±klayacaÄŸÄ±z
        response = scraper.get(article_url, timeout=10)
        soup = BeautifulSoup(response.text, 'lxml')
        
        # 1. Ã–ncelik: Meta Etiketi (En temiz link buradadÄ±r)
        meta_tag = soup.find("meta", {"name": "citation_pdf_url"})
        if meta_tag:
            return fix_url(meta_tag.get("content"))
        
        # 2. Ã–ncelik: Buton
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            if 'download/article-file' in link['href']:
                return fix_url(link['href'])
                
    except Exception as e:
        # Hata olsa bile en azÄ±ndan makale linkini dÃ¶ndÃ¼r, kullanÄ±cÄ± oradan indirsin
        return None
    return None

# ========================================================
# ARAYÃœZ
# ========================================================
st.title("ğŸŒ Harici Kaynaklar & CanlÄ± Arama")
tab1, tab2 = st.tabs(["ğŸ“œ HTU ArÅŸivi", "ğŸ¤– DergiPark Botu"])

# --- SEKME 1: HTU ---
with tab1:
    st.header("ğŸ“œ HTU Dijital SÃ¼reli YayÄ±nlar")
    col1, col2 = st.columns([4,1])
    search_term = col1.text_input("HTU YayÄ±nÄ± Ara (NO veya Ä°sim):", placeholder="Ã–rn: 2662, Tanin...")
    
    with st.spinner("TÃ¼m arÅŸiv taranÄ±yor..."):
        df = htu_verilerini_getir()
    
    if not df.empty:
        if search_term:
            df = df[
                df['BAÅLIK'].str.contains(search_term, case=False) | 
                df['HTU NO.'].str.contains(search_term, case=False) |
                df['AÃ‡IKLAMA'].str.contains(search_term, case=False)
            ]
        st.success(f"Toplam {len(df)} kayÄ±t listelendi.")
        df.insert(0, "SeÃ§", False)
        edited_df = st.data_editor(
            df,
            column_config={
                "SeÃ§": st.column_config.CheckboxColumn("Ä°ndir", default=False),
                "LINK": st.column_config.LinkColumn("GÃ¶rÃ¼ntÃ¼le")
            },
            hide_index=True, use_container_width=True, key="htu_editor"
        )
        selected_rows = edited_df[edited_df["SeÃ§"] == True]
        if not selected_rows.empty and st.button("ğŸ“¦ SeÃ§ilenleri Ä°ndir (ZIP)", type="primary"):
            progress_bar = st.progress(0)
            zip_buffer = BytesIO()
            with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zf:
                for idx, row in enumerate(selected_rows.itertuples()):
                    safe_title = re.sub(r'[\\/*?:"<>|]', "", row.BAÅLIK)[:40]
                    safe_filename = f"{row._2}_{safe_title}" 
                    if row.LINK.endswith(".djvu"):
                        c, m = download_and_process_djvu(row.LINK, safe_filename)
                        if c: zf.writestr(f"{safe_filename}.djvu", c)
                        else: zf.writestr(f"{safe_filename}_HATA.txt", m)
                    else:
                        zf.writestr(f"{safe_filename}_LINK.txt", f"Link: {row.LINK}")
                    progress_bar.progress((idx + 1) / len(selected_rows))
            st.download_button("ğŸ’¾ ZIP Kaydet", zip_buffer.getvalue(), "HTU_Arsiv.zip", "application/zip")

# --- SEKME 2: DERGÄ°PARK (LÄ°NK MODU) ---
with tab2:
    st.header("ğŸ¤– DergiPark Makale AvcÄ±sÄ±")
    with st.form("dp_form"):
        col1, col2 = st.columns([4,1])
        dp_kelime = col1.text_input("Makale Ara:", placeholder="Ã–rn: Milli MÃ¼cadele...")
        dp_btn = col2.form_submit_button("ğŸš€ Ara")

    if 'dp_results' not in st.session_state:
        st.session_state.dp_results = []
    
    # Bulunan linkleri hafÄ±zada tutmak iÃ§in
    if 'found_links' not in st.session_state:
        st.session_state.found_links = {}

    if dp_btn and dp_kelime:
        st.session_state.found_links = {} # Yeni aramada temizle
        with st.spinner("ğŸ¦ Brave arÅŸivleri tarÄ±yor..."):
            st.session_state.dp_results = search_dergipark_brave(dp_kelime)

    if st.session_state.dp_results:
        st.success(f"âœ… {len(st.session_state.dp_results)} makale bulundu.")
        
        for i, makale in enumerate(st.session_state.dp_results):
            with st.expander(f"ğŸ“„ {makale['title']}"):
                st.write(f"_{makale['desc']}_")
                
                col_a, col_b = st.columns([1, 3])
                unique_key = f"dp_{i}"
                
                with col_a:
                    # EÄŸer linki daha Ã¶nce bulduysak direkt butonu gÃ¶ster
                    if unique_key in st.session_state.found_links:
                        final_link = st.session_state.found_links[unique_key]
                        st.link_button("ğŸ“¥ PDF'Ä° Ä°NDÄ°R (Yeni Sekme)", final_link, type="primary")
                    
                    # HenÃ¼z bulmadÄ±ysak "HazÄ±rla" butonu gÃ¶ster
                    else:
                        if st.button("ğŸ” PDF Linkini Bul", key=f"btn_{unique_key}"):
                            with st.spinner("Link Ã§Ã¶zÃ¼mleniyor..."):
                                pdf_link = get_real_pdf_link(makale['link'])
                                
                                if pdf_link:
                                    st.session_state.found_links[unique_key] = pdf_link
                                    st.rerun() # SayfayÄ± yenile ve butonu getir
                                else:
                                    st.error("PDF linki gizli.")
                                    # Linki bulamazsa bari makale linkini verelim
                                    st.link_button("Siteye Git ve Ä°ndir", makale['link'])

                with col_b:
                    st.markdown(f"ğŸ‘‰ **[Makale SayfasÄ±na Git]({makale['link']})**")
    elif dp_btn:
        st.warning("SonuÃ§ bulunamadÄ±.")
