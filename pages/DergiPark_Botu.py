import streamlit as st
import requests
import pandas as pd
from io import BytesIO
import zipfile
from bs4 import BeautifulSoup
import urllib3
import urllib.parse # URL Encode iÃ§in gerekli

# SSL UyarÄ±larÄ±nÄ± Sustur
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(page_title="Harici Kaynaklar", page_icon="ğŸŒ", layout="wide")

# --- API ANAHTARI ---
# Not: Bu anahtarÄ±n kotasÄ± dolarsa kod "Manuel Ä°ndir" butonunu gÃ¶sterir.
ZENROWS_API_KEY = "6f09eed1a045e0384a2e8aa817a155f0ade82187"

# --- YAN MENÃœ ---
with st.sidebar:
    st.title("âš™ï¸ Kontrol Paneli")
    st.success("âœ… HTU: Agresif Mod")
    st.success("âœ… DergiPark: ZenRows + Manuel Fallback")
    st.markdown("---")

# --- URL DÃœZELTÄ°CÄ° ---
def fix_url(link):
    if not link: return ""
    if not link.startswith("http"):
        if link.startswith("dergipark") or link.startswith("www"):
            link = "https://" + link
        elif link.startswith("/"):
            link = "https://dergipark.org.tr" + link
    return link

# ========================================================
# 1. HTU ARÅÄ°VÄ° (STANDART)
# ========================================================
@st.cache_data(ttl=3600)
def htu_verilerini_getir():
    base_url = "https://www.tufs.ac.jp/common/fs/asw/tur/htu/"
    pages = ["list1.html", "list2.html"] 
    headers = {'User-Agent': 'Mozilla/5.0'}
    all_data = []
    
    for page in pages:
        full_url = base_url + page
        try:
            r = requests.get(full_url, headers=headers, timeout=30, verify=False)
            r.encoding = 'utf-8' 
            if r.status_code == 200:
                try: soup = BeautifulSoup(r.content, 'lxml')
                except: soup = BeautifulSoup(r.content, 'html.parser')
                all_rows = soup.find_all('tr')
                for row in all_rows:
                    try:
                        cols = row.find_all(['td', 'th'])
                        if len(cols) >= 3:
                            col_texts = [c.get_text(strip=True) for c in cols]
                            htu_no = col_texts[1]
                            if "HTU NO" in htu_no or not htu_no: continue
                            if len(htu_no) > 20: continue
                            baslik = col_texts[2] if len(cols) > 2 else ""
                            aciklama = col_texts[3] if len(cols) > 3 else ""
                            link_tag = cols[2].find('a')
                            raw_link = link_tag['href'] if link_tag and link_tag.has_attr('href') else ""
                            full_link = base_url + raw_link if raw_link and not raw_link.startswith("http") else raw_link
                            all_data.append({"HTU NO.": htu_no, "BAÅLIK": baslik, "AÃ‡IKLAMA": aciklama, "LINK": full_link})
                    except: continue
        except Exception as e: st.error(f"HTU HatasÄ±: {e}")
    df = pd.DataFrame(all_data)
    if not df.empty: df = df.drop_duplicates(subset=['HTU NO.'])
    return df

def download_and_process_djvu(url, filename):
    try:
        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, stream=True, verify=False)
        return (r.content, "OK") if r.status_code == 200 else (None, "BulunamadÄ±")
    except Exception as e: return None, str(e)


# ========================================================
# 2. DERGÄ°PARK (ZENROWS + MANUEL YEDEK)
# ========================================================

def search_dergipark_brave(keyword, count=15):
    try: api_key = st.secrets["BRAVE_API_KEY"]
    except: st.error("âš ï¸ Brave API AnahtarÄ± eksik!"); return []

    url = "https://api.search.brave.com/res/v1/web/search"
    query = f'site:dergipark.org.tr/tr/pub "{keyword}"'
    headers = {"Accept": "application/json", "Accept-Encoding": "gzip", "X-Subscription-Token": api_key}
    params = {"q": query, "count": count, "country": "tr"}
    
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            data = response.json()
            results = []
            if "web" in data and "results" in data["web"]:
                for item in data["web"]["results"]:
                    raw_link = item["url"]
                    clean_link = fix_url(raw_link)
                    if "dergipark.org.tr" in clean_link and "/pub/article/" not in clean_link:
                        results.append({
                            "title": item["title"],
                            "link": clean_link,
                            "desc": item.get("description", "")
                        })
            return results
    except Exception as e: st.error(f"Arama HatasÄ±: {e}")
    return []

def fetch_pdf_zenrows_direct(article_url):
    """
    ZenRows API ile PDF indirir. Hata alÄ±rsa 'MANUAL_MODE' dÃ¶ndÃ¼rÃ¼r.
    BÃ¶ylece kullanÄ±cÄ± butona basÄ±p kendi indirebilir.
    """
    status_box = st.empty()
    status_box.info("ğŸš€ ZenRows ile baÄŸlanÄ±lÄ±yor...")
    
    zenrows_url = "https://api.zenrows.com/v1/"
    
    # 1. ADIM: URL'yi GÃ¼venli Hale Getir (Encode)
    # ?issue_id=... gibi parametrelerin ZenRows'u bozmasÄ±nÄ± engeller.
    encoded_url = urllib.parse.quote(article_url, safe='') 

    params_html = {
        'url': article_url, # Requests kÃ¼tÃ¼phanesi bunu halleder ama biz garantiye alalÄ±m
        'apikey': ZENROWS_API_KEY,
        'js_render': 'true',
        'antibot': 'true',
        'premium_proxy': 'true', # 404 hatalarÄ±nÄ± azaltmak iÃ§in
        'country': 'tr' # TÃ¼rkiye IP'si kullanmaya Ã§alÄ±ÅŸ
    }
    
    try:
        # Siteye Git (HTML Ã‡ek)
        response = requests.get(zenrows_url, params=params_html)
        
        # EÄŸer ZenRows hata verirse (404, 403, Kota Bitti vb.)
        if response.status_code != 200:
            status_box.warning(f"Otomatik indirme yapÄ±lamadÄ± (Hata: {response.status_code}). Manuel moda geÃ§iliyor...")
            return "MANUAL_MODE", None # Hata durumunda manuel moda geÃ§
            
        soup = BeautifulSoup(response.text, 'lxml')
        
        # PDF Linkini Bul
        pdf_link = None
        meta_tag = soup.find("meta", {"name": "citation_pdf_url"})
        if meta_tag:
            pdf_link = meta_tag.get("content")
        else:
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                if 'download/article-file' in link['href']:
                    pdf_link = link['href']
                    break
        
        if not pdf_link:
            status_box.warning("PDF linki sayfada bulunamadÄ±. Siteye gitmeyi deneyin.")
            return "MANUAL_MODE", None
            
        final_pdf_url = fix_url(pdf_link)
        status_box.info(f"âœ… Link bulundu! Ä°ndiriliyor...")
        
        # 2. ADIM: PDF'i Ä°ndir
        params_pdf = {
            'url': final_pdf_url,
            'apikey': ZENROWS_API_KEY,
            'antibot': 'true',
        }
        
        pdf_response = requests.get(zenrows_url, params=params_pdf)
        
        if pdf_response.status_code == 200:
            status_box.empty()
            return "SUCCESS", pdf_response.content
        else:
            status_box.warning("PDF indirilirken baÄŸlantÄ± koptu. Manuel moda geÃ§iliyor.")
            return "MANUAL_MODE", final_pdf_url

    except Exception as e:
        status_box.error(f"Sistem HatasÄ±: {e}")
        return "MANUAL_MODE", None

# ========================================================
# ARAYÃœZ
# ========================================================
st.title("ğŸŒ Harici Kaynaklar & CanlÄ± Arama")
tab1, tab2 = st.tabs(["ğŸ“œ HTU ArÅŸivi", "ğŸ¤– DergiPark Botu"])

# --- SEKME 1: HTU ---
with tab1:
    st.header("ğŸ“œ HTU Dijital SÃ¼reli YayÄ±nlar")
    col1, col2 = st.columns([4,1])
    search_term = col1.text_input("HTU YayÄ±nÄ± Ara (NO veya Ä°sim):", placeholder="Ã–rn: 2662, Tanin...")
    
    with st.spinner("TÃ¼m arÅŸiv taranÄ±yor..."):
        df = htu_verilerini_getir()
    
    if not df.empty:
        if search_term:
            df = df[
                df['BAÅLIK'].str.contains(search_term, case=False) | 
                df['HTU NO.'].str.contains(search_term, case=False) |
                df['AÃ‡IKLAMA'].str.contains(search_term, case=False)
            ]
        st.success(f"Toplam {len(df)} kayÄ±t listelendi.")
        df.insert(0, "SeÃ§", False)
        edited_df = st.data_editor(
            df,
            column_config={
                "SeÃ§": st.column_config.CheckboxColumn("Ä°ndir", default=False),
                "LINK": st.column_config.LinkColumn("GÃ¶rÃ¼ntÃ¼le")
            },
            hide_index=True, use_container_width=True, key="htu_editor"
        )
        selected_rows = edited_df[edited_df["SeÃ§"] == True]
        if not selected_rows.empty and st.button("ğŸ“¦ SeÃ§ilenleri Ä°ndir (ZIP)", type="primary"):
            progress_bar = st.progress(0)
            zip_buffer = BytesIO()
            with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zf:
                for idx, row in enumerate(selected_rows.itertuples()):
                    safe_title = re.sub(r'[\\/*?:"<>|]', "", row.BAÅLIK)[:40]
                    safe_filename = f"{row._2}_{safe_title}" 
                    if row.LINK.endswith(".djvu"):
                        c, m = download_and_process_djvu(row.LINK, safe_filename)
                        if c: zf.writestr(f"{safe_filename}.djvu", c)
                        else: zf.writestr(f"{safe_filename}_HATA.txt", m)
                    else:
                        zf.writestr(f"{safe_filename}_LINK.txt", f"Link: {row.LINK}")
                    progress_bar.progress((idx + 1) / len(selected_rows))
            st.download_button("ğŸ’¾ ZIP Kaydet", zip_buffer.getvalue(), "HTU_Arsiv.zip", "application/zip")

# --- SEKME 2: DERGÄ°PARK (ZENROWS + FALLBACK) ---
with tab2:
    st.header("ğŸ¤– DergiPark Makale AvcÄ±sÄ±")
    with st.form("dp_form"):
        col1, col2 = st.columns([4,1])
        dp_kelime = col1.text_input("Makale Ara:", placeholder="Ã–rn: Milli MÃ¼cadele...")
        dp_btn = col2.form_submit_button("ğŸš€ Ara")

    if 'dp_results' not in st.session_state:
        st.session_state.dp_results = []
    
    if 'dergipark_cache' not in st.session_state:
        st.session_state.dergipark_cache = {}

    if dp_btn and dp_kelime:
        with st.spinner("ğŸ¦ Brave arÅŸivleri tarÄ±yor..."):
            st.session_state.dp_results = search_dergipark_brave(dp_kelime)

    if st.session_state.dp_results:
        st.success(f"âœ… {len(st.session_state.dp_results)} makale bulundu.")
        
        for i, makale in enumerate(st.session_state.dp_results):
            with st.expander(f"ğŸ“„ {makale['title']}"):
                st.write(f"_{makale['desc']}_")
                
                col_a, col_b = st.columns([1, 3])
                unique_key = f"dp_{i}"
                
                with col_a:
                    # Dosya zaten hafÄ±zadaysa indir
                    if unique_key in st.session_state.dergipark_cache:
                        data, mode, link = st.session_state.dergipark_cache[unique_key]
                        
                        if mode == "SUCCESS":
                            clean_name = re.sub(r'[\\/*?:"<>|]', "", makale['title'])[:30] + ".pdf"
                            st.download_button("ğŸ’¾ PDF Ä°NDÄ°R", data, clean_name, "application/pdf", key=f"dl_{unique_key}", type="primary")
                        elif mode == "MANUAL":
                            # ZenRows patladÄ±ysa burasÄ± Ã§alÄ±ÅŸÄ±r
                            st.warning("Bot korumasÄ± aÅŸÄ±lamadÄ±.")
                            st.link_button("ğŸ“¥ PDF'Ä° KENDÄ°N Ä°NDÄ°R", link if link else makale['link'], type="primary")
                    
                    # HenÃ¼z iÅŸlem yapÄ±lmadÄ±ysa
                    else:
                        if st.button("ğŸ“¥ PDF HazÄ±rla", key=f"btn_{unique_key}"):
                            status, content = fetch_pdf_zenrows_direct(makale['link'])
                            
                            if status == "SUCCESS":
                                st.session_state.dergipark_cache[unique_key] = (content, "SUCCESS", None)
                                st.rerun()
                            else:
                                # Hata aldÄ±ysak, bulunan linki (varsa) veya ana linki kaydet
                                st.session_state.dergipark_cache[unique_key] = (None, "MANUAL", content) # content burada link oluyor
                                st.rerun()

                with col_b:
                    st.markdown(f"ğŸ‘‰ **[Siteye Git]({makale['link']})**")
    elif dp_btn:
        st.warning("SonuÃ§ bulunamadÄ±.")
